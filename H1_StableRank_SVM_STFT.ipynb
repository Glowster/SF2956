{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad366f7",
   "metadata": {},
   "source": [
    "\n",
    "# H1 Stable Rank (via Subsampling) + SVM on STFT Point Clouds\n",
    "\n",
    "This notebook mirrors the style of *lecture_15* workflows, but applies them to your **STFT** data:\n",
    "1. Treat each spectrogram (**1000×257**) as a **point cloud** of 1000 points in 257D (each time frame is a point).\n",
    "2. Compute **H1 stable rank** using **subsampling** + **Vietoris–Rips persistence** (via `ripser`).\n",
    "3. Train an **SVM** on the resulting **H1 stable rank features** (one feature vector per clip).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e2ca6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If imports fail, run the install cell below.\n",
    "import os, json, glob, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# TDA / ML\n",
    "from ripser import ripser                # pip install ripser\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57f85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install helpers if needed (uncomment the relevant lines in your environment)\n",
    "# %pip install ripser scikit-learn tqdm joblib\n",
    "# Conda (alternative):\n",
    "# !conda install -y -c conda-forge ripser scikit-learn tqdm joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6aa37",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration\n",
    "Set where your `stft_np/` lives (created by your earlier STFT script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4718de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Paths / config ===\n",
    "BASE_DIR      = '.'\n",
    "META_PATH     = os.path.join(BASE_DIR, 'stft_np', 'meta.json')\n",
    "A_MEMMAP_PATH = None\n",
    "B_MEMMAP_PATH = None\n",
    "\n",
    "# Subsample strategy for Stable Rank\n",
    "SUBSAMPLE_SIZE    = 150     # m points per subsample\n",
    "N_SUBSAMPLES      = 6       # subsamples per clip\n",
    "T_POINTS          = 64      # number of t-grid points for the stable-rank curve\n",
    "RANDOM_STATE      = 42\n",
    "\n",
    "# Dataset usage (adjust for speed)\n",
    "N_PER_CLASS       = 1000     # clips per class to use (raise when ready)\n",
    "TEST_SIZE         = 0.2\n",
    "\n",
    "# Pre-scan parameters to set global t-grid range\n",
    "PRESCAN_SAMPLES   = 100     # number of clips to probe for t_max estimate\n",
    "PRESCAN_SUBS      = 2\n",
    "PRESCAN_M         = 100\n",
    "\n",
    "# Homology coefficient field (prime only): 2, 3, 5, 7, ...\n",
    "COEFF = 2\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"Configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37947af9",
   "metadata": {},
   "source": [
    "\n",
    "## Load STFT arrays and pick subsets\n",
    "Each clip is `(1000, 257)` in **dB** ([-80, 0]). We will convert each to a point cloud in `[0,1]` or z-scored space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9691ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: (940, 1000, 257) B: (973, 1000, 257)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Load meta & memmaps ===\n",
    "with open(META_PATH, 'r') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "dtype_str = meta.get('dtype', 'float16')\n",
    "if isinstance(dtype_str, str) and dtype_str.startswith(\"<class 'numpy.\"):\n",
    "    dtype_str = dtype_str.split('.')[-1].split(\"'\")[0]\n",
    "\n",
    "A_MEMMAP_PATH = os.path.join(BASE_DIR, 'stft_np', meta['A_memmap'])\n",
    "B_MEMMAP_PATH = os.path.join(BASE_DIR, 'stft_np', meta['B_memmap'])\n",
    "A_shape = tuple(meta['A_shape'])\n",
    "B_shape = tuple(meta['B_shape'])\n",
    "\n",
    "A_mm = np.memmap(A_MEMMAP_PATH, dtype=dtype_str, mode='r', shape=A_shape)\n",
    "B_mm = np.memmap(B_MEMMAP_PATH, dtype=dtype_str, mode='r', shape=B_shape)\n",
    "\n",
    "print(\"A:\", A_mm.shape, \"B:\", B_mm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e78613ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 940 from A and 973 from B → total 1913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Choose subset indices and labels ===\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "nA, nB = A_mm.shape[0], B_mm.shape[0]\n",
    "nA_use = min(N_PER_CLASS, nA)\n",
    "nB_use = min(N_PER_CLASS, nB)\n",
    "\n",
    "idxA = rng.choice(nA, size=nA_use, replace=False)\n",
    "idxB = rng.choice(nB, size=nB_use, replace=False)\n",
    "\n",
    "labels = np.concatenate([np.zeros(nA_use, dtype=int), np.ones(nB_use, dtype=int)])\n",
    "print(f\"Using {nA_use} from A and {nB_use} from B → total {labels.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d8de4",
   "metadata": {},
   "source": [
    "\n",
    "## Point cloud conversion\n",
    "We treat each **time frame** (length-257 vector) as one point. We map dB to [0,1] and then **z-score per clip** (across the 1000 frames) so distances are meaningful. Finally, we optionally **L2-normalize** per point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35cde75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point-cloud converter ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def spectrogram_to_point_cloud(db_img, zscore=True, l2norm=True):\n",
    "    \"\"\"db_img: (1000, 257) in dB [-80,0].\n",
    "    Returns a (1000, 257) float32 point cloud.\n",
    "    \"\"\"\n",
    "    # Map to [0,1]\n",
    "    x = (db_img + 80.0) / 80.0\n",
    "    x = np.clip(x, 0.0, 1.0).astype(np.float32, copy=False)  # (T=1000, F=257)\n",
    "\n",
    "    if zscore:\n",
    "        mu = x.mean(axis=0, keepdims=True)\n",
    "        sd = x.std(axis=0, keepdims=True) + 1e-6\n",
    "        x = (x - mu) / sd\n",
    "\n",
    "    if l2norm:\n",
    "        nrm = np.linalg.norm(x, axis=1, keepdims=True) + 1e-9\n",
    "        x = x / nrm\n",
    "\n",
    "    return x.astype(np.float32, copy=False)\n",
    "\n",
    "# quick smoke test on one clip\n",
    "_ = spectrogram_to_point_cloud(np.array(A_mm[0], dtype=np.float32))\n",
    "print(\"Point-cloud converter ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c686c",
   "metadata": {},
   "source": [
    "\n",
    "## Vietoris–Rips H1 diagrams and **Stable Rank** via subsampling\n",
    "**Stable rank curve** here means: for a grid of radii `t`, average across subsamples the **Betti-1** rank (number of H1 intervals alive at `t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45b318c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable-rank helpers ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def h1_diagram(points, maxdim=1):\n",
    "    \"\"\"Compute H1 diagram for a point cloud using ripser.\n",
    "    Returns an (n_pairs, 2) array [birth, death].\n",
    "    \"\"\"\n",
    "    # ripser expects (n_points, n_dims), metric='euclidean' by default\n",
    "    R = ripser(points, maxdim=maxdim, coeff=COEFF)\n",
    "    dgms = R['dgms']\n",
    "    H1 = dgms[1] if len(dgms) > 1 else np.empty((0,2), dtype=np.float64)\n",
    "    # ensure finite\n",
    "    if H1.size:\n",
    "        # Replace inf deaths with large sentinel (max finite death * 1.05)\n",
    "        finite = np.isfinite(H1[:,1])\n",
    "        if finite.any():\n",
    "            m = H1[finite,1].max()\n",
    "            H1[~finite,1] = m * 1.05\n",
    "        else:\n",
    "            H1 = np.empty((0,2), dtype=np.float64)\n",
    "    return H1\n",
    "\n",
    "def betti_curve(diag, t_vals):\n",
    "    \"\"\"Given H1 diagram (birth, death), compute Betti-1(t) for each t.\n",
    "    Returns an array of shape (len(t_vals),).\n",
    "    \"\"\"\n",
    "    if diag.size == 0:\n",
    "        return np.zeros_like(t_vals, dtype=np.float32)\n",
    "    b = diag[:,0][:,None]  # (k,1)\n",
    "    d = diag[:,1][:,None]  # (k,1)\n",
    "    t = t_vals[None,:]     # (1,T)\n",
    "    alive = (b <= t) & (t < d)  # (k,T)\n",
    "    return alive.sum(axis=0).astype(np.float32)\n",
    "\n",
    "def stable_rank_via_subsampling(pc, t_vals, m=150, n_subs=6, rng=None):\n",
    "    N, D = pc.shape\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    # ensure rows >= cols to avoid Ripser warning\n",
    "    m_eff = min(N, max(m, D + 1))\n",
    "\n",
    "    acc = np.zeros_like(t_vals, dtype=np.float32)\n",
    "    for _ in range(n_subs):\n",
    "        idx = rng.choice(N, size=m_eff, replace=False)\n",
    "        diag = h1_diagram(pc[idx])\n",
    "        acc += betti_curve(diag, t_vals)\n",
    "    return (acc / n_subs).astype(np.float32)\n",
    "    \n",
    "print(\"Stable-rank helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca531c8",
   "metadata": {},
   "source": [
    "\n",
    "## Set a **global** radius grid `t`\n",
    "\n",
    "We estimate a **common** `t_max` by a small pre-scan across a few clips, then define `t_vals = linspace(0, t_max, T_POINTS)`. This aligns features across all samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e23b840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_max≈ 2.0 grid length: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def estimate_tmax(mmA, mmB, idxA, idxB, prescan_samples=100, subs=2, m=100, seed=RANDOM_STATE):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # pick subset of clips to scan\n",
    "    idsA = rng.choice(len(idxA), size=min(prescan_samples//2, len(idxA)), replace=False)\n",
    "    idsB = rng.choice(len(idxB), size=min(prescan_samples - len(idsA), len(idxB)), replace=False)\n",
    "    deaths = []\n",
    "    for i in idsA:\n",
    "        pc = spectrogram_to_point_cloud(np.array(mmA[idxA[i]], dtype=np.float32))\n",
    "        for _ in range(subs):\n",
    "            I = rng.choice(pc.shape[0], size=min(m, pc.shape[0]), replace=False)\n",
    "            H1 = h1_diagram(pc[I])\n",
    "            if H1.size:\n",
    "                deaths.append(np.max(H1[:,1]))\n",
    "    for i in idsB:\n",
    "        pc = spectrogram_to_point_cloud(np.array(mmB[idxB[i]], dtype=np.float32))\n",
    "        for _ in range(subs):\n",
    "            I = rng.choice(pc.shape[0], size=min(m, pc.shape[0]), replace=False)\n",
    "            H1 = h1_diagram(pc[I])\n",
    "            if H1.size:\n",
    "                deaths.append(np.max(H1[:,1]))\n",
    "    if len(deaths) == 0:\n",
    "        return 1.0\n",
    "    deaths = np.array(deaths, dtype=np.float64)\n",
    "    tmax = float(np.quantile(deaths, 0.95))\n",
    "    return max(tmax, 1e-3)\n",
    "\n",
    "t_max = 2.0 #estimate_tmax(A_mm, B_mm, idxA, idxB, prescan_samples=PRESCAN_SAMPLES, subs=PRESCAN_SUBS, m=PRESCAN_M)\n",
    "t_vals = np.linspace(0.0, t_max, T_POINTS).astype(np.float32)\n",
    "print(\"t_max≈\", t_max, \"grid length:\", len(t_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da6496",
   "metadata": {},
   "source": [
    "\n",
    "## Compute H1 stable rank features\n",
    "One **(T_POINTS, )** feature vector **per clip**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93bf7cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing A features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A stable-rank: 100%|██████████| 940/940 [00:37<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing B features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B stable-rank: 100%|██████████| 973/973 [00:31<00:00, 31.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1913, 64) y: (1913,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def features_for_indices(mm, indices, t_vals, m=SUBSAMPLE_SIZE, n_subs=N_SUBSAMPLES, n_jobs=-1, desc='features'):\n",
    "    def one(idx):\n",
    "        db = np.array(mm[idx], dtype=np.float32)\n",
    "        pc = spectrogram_to_point_cloud(db)\n",
    "        return stable_rank_via_subsampling(pc, t_vals, m=m, n_subs=n_subs, rng=np.random.default_rng(RANDOM_STATE+idx))\n",
    "    out = Parallel(n_jobs=n_jobs, prefer='processes')(\n",
    "        delayed(one)(i) for i in tqdm(indices, desc=desc)\n",
    "    )\n",
    "    return np.vstack(out).astype(np.float32, copy=False)\n",
    "\n",
    "print(\"Computing A features...\")\n",
    "XA = features_for_indices(A_mm, idxA, t_vals, desc='A stable-rank')\n",
    "print(\"Computing B features...\")\n",
    "XB = features_for_indices(B_mm, idxB, t_vals, desc='B stable-rank')\n",
    "\n",
    "X = np.vstack([XA, XB]).astype(np.float32, copy=False)\n",
    "y = labels.copy()\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7db9b",
   "metadata": {},
   "source": [
    "\n",
    "## Train / Test split and SVM fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40516520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6762402088772846\n",
      "F1 (weighted): 0.6762666959068283\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.67      0.68      0.67       188\n",
      "           B       0.69      0.67      0.68       195\n",
      "\n",
      "    accuracy                           0.68       383\n",
      "   macro avg       0.68      0.68      0.68       383\n",
      "weighted avg       0.68      0.68      0.68       383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "train_idx, test_idx = next(sss.split(X, y))\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', C=10.0, gamma='scale', probability=False, random_state=RANDOM_STATE)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 (weighted):\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['A','B']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa1818",
   "metadata": {},
   "source": [
    "\n",
    "## Save features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69284f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to stft_np/tda_features/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs('stft_np/tda_features', exist_ok=True)\n",
    "np.save('stft_np/tda_features/X_stable_rank.npy', X)\n",
    "np.save('stft_np/tda_features/y_labels.npy', y)\n",
    "np.save('stft_np/tda_features/t_grid.npy', t_vals)\n",
    "print(\"Saved to stft_np/tda_features/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40faf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31049a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
