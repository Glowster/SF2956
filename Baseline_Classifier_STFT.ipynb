{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17eef92",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Classifier on STFT Spectrograms (Non‑TDA)\n",
    "\n",
    "This notebook provides a **standard baseline** to compare against your homological approach.\n",
    "\n",
    "**Pipeline**\n",
    "1. Load `stft_np/` memmaps (A/B classes).\n",
    "2. Extract **simple summary features** from each spectrogram (per‑frequency mean & std across time, plus first‑difference mean & std).\n",
    "3. Train/test split.\n",
    "4. Train a **Linear SVM** and print **test accuracy** (and F1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a53a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If an import fails, run the install cell below and re-run.\n",
    "import os, json, numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964baf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -q scikit-learn joblib tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411876bb",
   "metadata": {},
   "source": [
    "\n",
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3289686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR     = '.'\n",
    "META_PATH    = os.path.join(BASE_DIR, 'stft_np', 'meta.json')\n",
    "\n",
    "# Limit how many clips per class (A/B) to load (adjust as you like)\n",
    "N_PER_CLASS  = 1000\n",
    "TEST_SIZE    = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f3705",
   "metadata": {},
   "source": [
    "\n",
    "## Load arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc985a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (940, 1000, 257) B shape: (973, 1000, 257)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(META_PATH, 'r') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "dtype_str = meta.get('dtype', 'float16')\n",
    "if isinstance(dtype_str, str) and dtype_str.startswith(\"<class 'numpy.\"):\n",
    "    dtype_str = dtype_str.split('.')[-1].split(\"'\")[0]\n",
    "\n",
    "A_path = os.path.join(BASE_DIR, 'stft_np', meta['A_memmap'])\n",
    "B_path = os.path.join(BASE_DIR, 'stft_np', meta['B_memmap'])\n",
    "A_shape = tuple(meta['A_shape'])\n",
    "B_shape = tuple(meta['B_shape'])\n",
    "\n",
    "A = np.memmap(A_path, dtype=dtype_str, mode='r', shape=A_shape)\n",
    "B = np.memmap(B_path, dtype=dtype_str, mode='r', shape=B_shape)\n",
    "\n",
    "print(\"A shape:\", A.shape, \"B shape:\", B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114b783",
   "metadata": {},
   "source": [
    "\n",
    "## Features: time‑summary stats per frequency bin\n",
    "\n",
    "For each clip `X` with shape `(T=1000, F=257)` in dB ([-80, 0]):\n",
    "\n",
    "- Convert to `[0,1]`: `Y = clip((X+80)/80)`  \n",
    "- Per frequency bin (across time): compute **mean** and **std** → `2*F` dims  \n",
    "- On first temporal difference `ΔY` (framewise diff): **mean** and **std** → `2*F` dims  \n",
    "- Total feature length = **4×F = 1028**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68ca7fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A features: 100%|██████████| 940/940 [00:00<00:00, 967.16it/s] \n",
      "B features: 100%|██████████| 973/973 [00:00<00:00, 1339.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (1913, 1028) Labels: (1913,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def spec_to_features(db_spec):\n",
    "    # db_spec: (T, F) dB in [-80,0]\n",
    "    Y = np.clip((db_spec.astype(np.float32) + 80.0) / 80.0, 0.0, 1.0)   # (T,F)\n",
    "    mu  = Y.mean(axis=0)                                                # (F,)\n",
    "    sd  = Y.std(axis=0)                                                 # (F,)\n",
    "    dY  = np.diff(Y, axis=0)                                            # (T-1,F)\n",
    "    dmu = dY.mean(axis=0)                                               # (F,)\n",
    "    dsd = dY.std(axis=0)                                                # (F,)\n",
    "    return np.concatenate([mu, sd, dmu, dsd], axis=0)                   # (4F,)\n",
    "\n",
    "def build_dataset(A, B, n_per_class=600, seed=42, n_jobs=-1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    nA = min(n_per_class, A.shape[0])\n",
    "    nB = min(n_per_class, B.shape[0])\n",
    "    idxA = rng.choice(A.shape[0], size=nA, replace=False)\n",
    "    idxB = rng.choice(B.shape[0], size=nB, replace=False)\n",
    "\n",
    "    def one(mm, idx):\n",
    "        x = np.array(mm[idx], dtype=np.float32)   # (T,F)\n",
    "        return spec_to_features(x)\n",
    "\n",
    "    XA = Parallel(n_jobs=n_jobs, prefer='threads')(\n",
    "        delayed(one)(A, i) for i in tqdm(idxA, desc=\"A features\")\n",
    "    )\n",
    "    XB = Parallel(n_jobs=n_jobs, prefer='threads')(\n",
    "        delayed(one)(B, i) for i in tqdm(idxB, desc=\"B features\")\n",
    "    )\n",
    "    X = np.vstack([XA, XB]).astype(np.float32)\n",
    "    y = np.concatenate([np.zeros(nA, dtype=int), np.ones(nB, dtype=int)])\n",
    "    return X, y\n",
    "\n",
    "X, y = build_dataset(A, B, n_per_class=N_PER_CLASS, seed=RANDOM_STATE)\n",
    "print(\"Feature matrix:\", X.shape, \"Labels:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bb594",
   "metadata": {},
   "source": [
    "\n",
    "## Train / Test split and Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bee07fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy: 0.7859\n",
      "Baseline Weighted F1 : 0.7859\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.78      0.79      0.78       188\n",
      "           B       0.79      0.78      0.79       195\n",
      "\n",
      "    accuracy                           0.79       383\n",
      "   macro avg       0.79      0.79      0.79       383\n",
      "weighted avg       0.79      0.79      0.79       383\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theodorbjork/SF2956/.venv/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "train_idx, test_idx = next(sss.split(X, y))\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LinearSVC(C=1.0, loss='hinge', random_state=RANDOM_STATE, max_iter=5000)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1  = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Baseline Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Baseline Weighted F1 : {f1:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['A','B']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24380d02",
   "metadata": {},
   "source": [
    "\n",
    "## (Optional) Save features for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13092dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved X/y to stft_np/baseline_features/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs('stft_np/baseline_features', exist_ok=True)\n",
    "np.save('stft_np/baseline_features/X.npy', X)\n",
    "np.save('stft_np/baseline_features/y.npy', y)\n",
    "print(\"Saved X/y to stft_np/baseline_features/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
